results were received from this training SVM with such params

verbose = False
max_iter = 50

estimator = svm.SVC(
    kernel='rbf', verbose=verbose, max_iter=max_iter,
    decision_function_shape='ovr')

# Results for unscalled data
accuracy: 0.45, notes: SVM based on default mnist dataset
    prec: 0.65, recall: 0.45, f1: 0.46
accuracy: 0.56, notes: RMB based encodings, training with sigmoids
    prec: 0.58, recall: 0.55, f1: 0.55
accuracy: 0.54, notes: RBM based encodings, training with binary mode
    prec: 0.54, recall: 0.54, f1: 0.54
accuracy: 0.68, notes: Autoencoder based on sigmoid RBM
    prec: 0.70, recall: 0.68, f1: 0.68
accuracy: 0.67, notes: Autoencoder based on sigmoid RBM, binarized encodings
    prec: 0.69, recall: 0.67, f1: 0.68
accuracy: 0.61, notes: Autoencoder based on binary mode RBM
    prec: 0.64, recall: 0.60, f1: 0.61
accuracy: 0.63, notes: Autoencoder based on binary mode RBM, binarized encodings
    prec: 0.65, recall: 0.63, f1: 0.63

# Results for scalled data
# from sklearn import preprocessing
# data = preprocessing.scale(data)
accuracy: 0.74, notes: SVM based on default mnist dataset, scaled_data
    prec: 0.78, recall: 0.74, f1: 0.74
accuracy: 0.65, notes: RMB based encodings, training with sigmoids, scaled_data
    prec: 0.65, recall: 0.65, f1: 0.64
accuracy: 0.64, notes: RBM based encodings, training with binary mode, scaled_data
    prec: 0.65, recall: 0.63, f1: 0.62
accuracy: 0.77, notes: Autoencoder based on sigmoid RBM, scaled_data
    prec: 0.78, recall: 0.77, f1: 0.76
accuracy: 0.73, notes: Autoencoder based on sigmoid RBM, binarized encodings, scaled_data
    prec: 0.74, recall: 0.73, f1: 0.72
accuracy: 0.75, notes: Autoencoder based on binary mode RBM, scaled_data
    prec: 0.76, recall: 0.75, f1: 0.74
accuracy: 0.72, notes: Autoencoder based on binary mode RBM, binarized encodings, scaled_data
    prec: 0.72, recall: 0.72, f1: 0.71


with open('clusterization_results_50_iters.pkl', 'rb') as f:
    results_50 = pickle.load(f)

def get_mean_metrics(metrics):
    mean_prec = np.mean(metrics[0])
    mean_recall = np.mean(metrics[1])
    mean_f1 = np.mean(metrics[2])
    return mean_prec, mean_recall, mean_f1

print_metrics = False
for results, iters in zip([results_50, results_100, results_200], [50, 100, 200]):
    print("Iters: %d" % iters)
    for r in results:
        if 'scaled' not in r['notes']:
            print('accuracy: %.2f, notes: %s' % (r['accuracy'], r['notes']))
            if print_metrics:
                mean_metrics = get_mean_metrics(r['prec_recall_f_score'])
                print("    prec: %.2f, recall: %.2f, f1: %.2f" % (mean_metrics[0], mean_metrics[1], mean_metrics[2]))


Результаты от проверки векторов
Как я проверял результаты: брал сжатый вариант MNIST для training и test датасетов. После тренил SVM на training датасете и проверял accuracy на test датасете. По идеи это должно показать нам насколько равномерное и делимое пространство образуют наши embeddings.

SVM on 200 iters
1. accuracy: 0.68, notes: SVM based on default mnist dataset
2. accuracy: 0.67, notes: RMB based encodings, training with sigmoids
3. accuracy: 0.68, notes: RBM based encodings, training with binary mode
4. accuracy: 0.78, notes: Autoencoder based on sigmoid RBM
5. accuracy: 0.76, notes: Autoencoder based on sigmoid RBM, binarized encodings
6. accuracy: 0.74, notes: Autoencoder based on binary mode RBM
7. accuracy: 0.72, notes: Autoencoder based on binary mode RBM, binarized encodings

1. Модель натрениная на обычном мнисте без зжатия
2. Модель использует embeddings с натрениной RMB, без автоенкодера. RBM внутренние слои пропущены через sigmoid, embedding - бинарный код
3. embeddings с натрениной RMB, без автоенкодера. RBM внутренние слои пропущены через бинаризацию, embedding - бинарный код
4. Автоенкодер который строился поверх sigmoid RBM, финальный embedding - probability
5. Автоенкодер который строился поверх sigmoid RBM, финальный embedding - бинарный, полученый через срез probability по prob > 0.2 = 1, prob <= 0,2 = 0.
6. Автоенкодер который строился поверх binary RBM, финальный embedding - probability
7. Автоенкодер который строился поверх binary RBM, финальный embedding - бинарный, полученый через срез probability по prob > 0.2 = 1, prob <= 0,2 = 0.

Странно что модель которая тренилась с sigmoid показывает лучше результаты. Ну и яское дело варианты 5 и 7 чуть хуже - но там мы получаем явное бинарное сжатие
