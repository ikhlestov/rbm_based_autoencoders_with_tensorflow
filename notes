Should we train all RBM layers at one iteration, or first train fully first layer, after second and so on?

# Iside second RBM
1. For updates used not learning rate, but learning rate / batch size
2. hidden_encode not equal in both RBMs

# hiden neuron probability is formed as
hid_probability = tf.nn.sigmoid(
            tf.add(
                tf.matmul(visible_units, self.W),
                self.bias_hidden
            )
        )

# states are formed as probability + some random normal distribution
# states are 0 or 1
hid_states = activation(probs + random_distribution)
# ex:
tf.nn.relu(tf.sign(probs - rand))
tf.floor(probs + tf.random_uniform(tf.shape(probs), 0, 1))

# to get reconstruction we should multiply hiden probs by transposed weighted matrix
self.reconstruction = tf.nn.sigmoid(
            tf.add(
                tf.matmul(hidden_units, tf.transpose(self.W)),
                self.bias_visible
            )
        )

# we can calculate hiden state -> reconstruction many times

# for Contrastive divergence update rule we use to part
CD = positive - negative
# where positive part
positive = tf.matmul(tf.transpose(visible), hidden_states_0)
# negative part
negative = tf.matmul(tf.transpose(last_reconstr), hidden_state_last)
# hiden state should be after last reconstruction


Should we for negative part use reconstructed probabilities or 0/1 states?????? The same for hidden part...
